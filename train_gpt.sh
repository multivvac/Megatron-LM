torchrun --nproc_per_node=4 pretrain_gpt.py \
  --tensor-model-parallel-size 1 \
  --pipeline-model-parallel-size 1 \
  --num-layers 8 \
  --hidden-size 512 \
  --num-attention-heads 8 \
  --seq-length 512 \
  --max-position-embeddings 512 \
  --micro-batch-size 4 \
  --global-batch-size 128 \
  --train-iters 1000 \
  --lr 0.0005 \
  --lr-decay-style cosine \
  --no-gradient-accumulation-fusion \
  --min-lr 1e-5 \
  --lr-warmup-iters 2 \
  --optimizer adam \
  --weight-decay 0.01 \
  --clip-grad 1.0 \
  --train-iters 1000 \
  --eval-iters 0 \
  --eval-interval 1000 \
  --save-interval 1000 \
  --log-interval 10 \
  --num-workers 1 \
  --split 100,0,0 \
  --data-path data/tinyshk_gpt2_text_document \
  --vocab-file tokenizer/gpt2-vocab.json \
  --merge-file tokenizer/gpt2-merges.txt \
  --tokenizer-type GPT2BPETokenizer \
  --save checkpoints/tinyshk-gpt2
