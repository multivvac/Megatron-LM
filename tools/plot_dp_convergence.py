#!/usr/bin/env python3
"""
Data Parallelism Convergence Plot (TP1-PP1)
-------------------------------------------
Plots training curves with X=iteration and Y=lm loss, one line per GPU count
for TP1-PP1 runs based on CSVs generated by extract_training_times.py.

Usage:
    python3 tools/plot_dp_convergence.py
    python3 tools/plot_dp_convergence.py --logs-dir logs/plots --output logs/plots/dp_convergence.png

Optional:
    --max-iters N        Limit iterations plotted (e.g., first N points)
    --downsample K       Plot every K-th point to reduce clutter
    --title "..."        Custom plot title
"""

import argparse
import csv
import re
from pathlib import Path
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt


def parse_filename(filename: str) -> Dict[str, int] | None:
    m = re.match(r"gpt2-N(\d+)-G(\d+)-TP(\d+)-PP(\d+)_times\.csv", filename)
    if not m:
        return None
    return {
        "nodes": int(m.group(1)),
        "gpus_per_node": int(m.group(2)),
        "tp": int(m.group(3)),
        "pp": int(m.group(4)),
    }


def read_iter_loss(csv_path: Path) -> Tuple[List[int], List[float]]:
    iters: List[int] = []
    losses: List[float] = []
    with csv_path.open("r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            iters.append(int(row["iteration"]))
            losses.append(float(row["loss"]))
    return iters, losses


def main():
    parser = argparse.ArgumentParser(description="Plot iteration vs lm loss for TP1-PP1 runs")
    parser.add_argument("--logs-dir", type=str, default="logs/plots", help="Directory containing *_times.csv files")
    parser.add_argument("--output", type=str, default="logs/plots/data_parallelism_convergence.png", help="Output plot path")
    parser.add_argument("--max-iters", type=int, default=None, help="Limit plotted iterations to first N points")
    parser.add_argument("--downsample", type=int, default=1, help="Plot every K-th point (default: 1 = no downsampling)")
    parser.add_argument("--title", type=str, default="Convergence: Iteration vs LM Loss (TP1-PP1)", help="Plot title")
    args = parser.parse_args()

    logs_dir = Path(args.logs_dir)
    csv_files = list(logs_dir.glob("*-TP1-PP1_times.csv"))
    if not csv_files:
        print(f"No TP1-PP1 CSV files found in {logs_dir}")
        return

    # Group data by total GPU count
    data_by_gpus: Dict[int, Tuple[List[int], List[float]]] = {}

    for csv_path in csv_files:
        cfg = parse_filename(csv_path.name)
        if not cfg:
            print(f"Skipping {csv_path.name}: cannot parse config")
            continue
        total_gpus = cfg["nodes"] * cfg["gpus_per_node"]
        iters, losses = read_iter_loss(csv_path)

        # Downsample and trim
        if args.downsample > 1:
            iters = iters[::args.downsample]
            losses = losses[::args.downsample]
        if args.max_iters is not None:
            max_n = args.max_iters
            iters = iters[:max_n]
            losses = losses[:max_n]

        data_by_gpus[total_gpus] = (iters, losses)
        print(f"Loaded {csv_path.name}: {total_gpus} GPUs, {len(iters)} points")

    if not data_by_gpus:
        print("No data found to plot")
        return

    # Sort by GPU count for consistent legend order
    sorted_items = sorted(data_by_gpus.items(), key=lambda x: x[0])

    plt.figure(figsize=(10, 6))
    for gpus, (iters, losses) in sorted_items:
        plt.plot(iters, losses, label=f"{gpus} GPUs", linewidth=2)

    plt.xlabel("Iteration", fontsize=12)
    plt.ylabel("LM Loss", fontsize=12)
    plt.title(args.title, fontsize=14, fontweight="bold")
    plt.grid(True, alpha=0.3)
    plt.legend(title="Data Parallel GPUs")

    Path(args.output).parent.mkdir(parents=True, exist_ok=True)
    plt.tight_layout()
    plt.savefig(args.output, dpi=300, bbox_inches="tight")
    print(f"Plot saved to {args.output}")


if __name__ == "__main__":
    main()
